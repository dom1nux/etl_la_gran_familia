# ETL La Gran Familia ğŸ¢

Pipeline ETL completo y modular en Python para extraer, transformar y cargar datos desde una base OLTP SQL Server hacia un Data Mart dimensional. Este proyecto implementa un esquema estrella optimizado para anÃ¡lisis de ventas inmobiliarias.

## ğŸ¯ CaracterÃ­sticas Principales

- **Arquitectura Modular**: SeparaciÃ³n clara entre extracciÃ³n, transformaciÃ³n y carga
- **Esquema Dimensional**: Implementa modelo estrella con dimensiones y tabla de hechos
- **Mapeo AutomÃ¡tico de IDs**: Convierte automÃ¡ticamente IDs de OLTP a IDs del Data Mart
- **GestiÃ³n de Dependencias Moderna**: Utiliza `uv` para manejo rÃ¡pido de entornos virtuales
- **Calculo de MÃ©tricas**: Computa costos, ganancias y presupuestos a nivel de proyecto
- **ValidaciÃ³n de Datos**: Elimina duplicados y valida integridad referencial

## ğŸ“ Estructura del Proyecto

```
etl_la_gran_familia/
â”œâ”€â”€ etl/                    # MÃ³dulos ETL principales
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py         # ExtracciÃ³n de datos desde OLTP
â”‚   â”œâ”€â”€ transform.py       # Transformaciones y limpieza
â”‚   â”œâ”€â”€ load.py           # Carga hacia Data Mart
â”‚   â””â”€â”€ utils.py          # Utilidades de conexiÃ³n
â”œâ”€â”€ queries/               # Scripts SQL especializados
â”‚   â”œâ”€â”€ dim_formapago.sql # DimensiÃ³n forma de pago
â”‚   â”œâ”€â”€ dim_tiempo.sql    # DimensiÃ³n temporal
â”‚   â”œâ”€â”€ hechos_venta.sql  # Tabla de hechos con mÃ©tricas
â”‚   â””â”€â”€ select_clientes.sql
â”œâ”€â”€ data/                  # Scripts de definiciÃ³n de BD
â”‚   â”œâ”€â”€ GranFamiliaDB.sql    # Base OLTP
â”‚   â”œâ”€â”€ GranFamiliaMart.sql  # Data Mart dimensional
â”‚   â””â”€â”€ GranFamilia.bacpac   # Backup de datos
â”œâ”€â”€ tests/                 # Pruebas unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_etl.py
â”œâ”€â”€ main.py               # Orquestador principal ETL
â”œâ”€â”€ pyproject.toml        # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ uv.lock              # Lock file de dependencias
â””â”€â”€ README.md            # Este archivo
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.13+**: Lenguaje principal
- **Pandas**: ManipulaciÃ³n y anÃ¡lisis de datos
- **SQLAlchemy**: ORM y manejo de conexiones
- **PyODBC**: Driver para SQL Server
- **uv**: GestiÃ³n moderna de entornos virtuales
- **SQL Server**: Base de datos OLTP y Data Mart
- **unittest**: Framework de pruebas

## ğŸš€ InstalaciÃ³n RÃ¡pida

### 1. Instalar uv (si no lo tienes)
```bash
# Linux/macOS
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. Clonar y configurar el proyecto
```bash
git clone <url-del-repositorio>
cd etl_la_gran_familia

# Crear entorno virtual e instalar dependencias
uv sync

# Configurar variables de entorno
cp .env.template .env
# Edita .env con tus credenciales de SQL Server
```

### 3. Configurar bases de datos
```sql
-- Ejecutar en SQL Server Management Studio
-- 1. Crear base OLTP
sqlcmd -S tu_servidor -i data/GranFamiliaDB.sql

-- 2. Crear Data Mart
sqlcmd -S tu_servidor -i data/GranFamiliaMart.sql

-- 3. (Opcional) Restaurar datos de ejemplo
-- Restaurar GranFamilia.bacpac usando SSMS
```

## ğŸ® Uso del Sistema

### EjecuciÃ³n Principal
```bash
# Ejecutar pipeline ETL completo
uv run main.py
```

### Comandos Ãštiles con uv
```bash
# Agregar nueva dependencia
uv add pandas sqlalchemy

# Eliminar dependencia
uv remove package_name

# Ejecutar comando especÃ­fico
uv run python -m unittest tests.test_etl

# Verificar dependencias
uv tree
```

## ğŸ”„ Flujo ETL Detallado

### Fase 1: ExtracciÃ³n (EXTRACT)
- ConexiÃ³n a base OLTP SQL Server
- ExtracciÃ³n de tablas: Cliente, Inmueble, Empleado, Proyecto
- GeneraciÃ³n de dimensiones calculadas: FormaPago, Tiempo
- TransformaciÃ³n de nombres de columnas

### Fase 2: Carga de Dimensiones (LOAD)
- Limpieza de tablas existentes (respetando FKs)
- EliminaciÃ³n de IDs IDENTITY para regeneraciÃ³n automÃ¡tica
- Carga de todas las dimensiones al Data Mart

### Fase 3: Mapeo de IDs
- CreaciÃ³n de mapas ID_OLTP â†’ ID_MART
- EliminaciÃ³n de duplicados en dimensiones
- PreparaciÃ³n para mapeo de tabla de hechos

### Fase 4: Procesamiento de Hechos
- ExtracciÃ³n usando query compleja con mÃ©tricas calculadas
- CÃ¡lculo de costos, ganancias y presupuestos por proyecto
- InclusiÃ³n de claves de negocio para mapeo

### Fase 5: Mapeo Dimensional
- ConversiÃ³n de IDs OLTP a IDs del Data Mart
- Mapeo secuencial usando claves de negocio
- Manejo especial de dimensiÃ³n temporal

### Fase 6: ValidaciÃ³n y Limpieza
- EliminaciÃ³n de registros incompletos
- RemociÃ³n de duplicados por clave primaria compuesta
- ValidaciÃ³n de integridad referencial

### Fase 7: Carga Final
- InserciÃ³n de tabla de hechos en Data Mart
- ValidaciÃ³n de carga exitosa

## ğŸ“Š Esquema del Data Mart

### Dimensiones
- **DIM.Cliente**: InformaciÃ³n de clientes
- **DIM.Empleado**: Datos de empleados/vendedores
- **DIM.Proyecto**: Detalles de proyectos inmobiliarios
- **DIM.Inmueble**: Propiedades y caracterÃ­sticas
- **DIM.FormaPago**: MÃ©todos de pago
- **DIM.Tiempo**: DimensiÃ³n temporal con jerarquÃ­as

### Tabla de Hechos
- **DIM.Hechos_Venta**: Transacciones con mÃ©tricas calculadas
  - Montos: total, costo, ganancia
  - Presupuestos por proyecto
  - Referencias a todas las dimensiones

## ğŸ§ª Pruebas

### Ejecutar todas las pruebas
```bash
uv run -m unittest discover tests
```

### Pruebas especÃ­ficas
```bash
# Prueba de conexiÃ³n
uv run -m unittest tests.test_etl.TestETL.test_connection

# Prueba de extracciÃ³n
uv run -m unittest tests.test_etl.TestETL.test_extract_cliente

# Prueba de transformaciÃ³n
uv run -m unittest tests.test_etl.TestETL.test_transform_data
```

## ğŸ“ˆ MÃ©tricas y KPIs

El sistema calcula automÃ¡ticamente:
- **Costo por Proyecto**: Suma de costos hasta fecha de venta
- **Ganancia por Venta**: Monto total - costos del proyecto
- **Presupuesto vs Ejecutado**: ComparaciÃ³n presupuestal
- **AnÃ¡lisis Temporal**: Ventas por perÃ­odo

## ğŸ›¡ï¸ Consideraciones de Seguridad

- Variables de entorno para credenciales
- Conexiones seguras con SQL Server
- ValidaciÃ³n de datos de entrada
- Manejo de errores robusto

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Variables de Entorno (.env)
```env
# Base OLTP
DB_SERVER=tu_servidor
DB_DATABASE=GranFamilia
DB_USERNAME=tu_usuario
DB_PASSWORD=tu_password

# Data Mart
MART_SERVER=tu_servidor
MART_DATABASE=GranFamiliaMart
MART_USERNAME=tu_usuario
MART_PASSWORD=tu_password
```

### PersonalizaciÃ³n del Pipeline
El sistema es altamente configurable:
- Agregar nuevas dimensiones editando `main.py`
- Crear queries personalizadas en `queries/`
- Implementar transformaciones en `etl/transform.py`
- Extender validaciones en `etl/load.py`

## ğŸ› SoluciÃ³n de Problemas

### Errores Comunes
1. **Error de conexiÃ³n**: Verificar credenciales en `.env`
2. **Duplicados en hechos**: El sistema elimina automÃ¡ticamente duplicados
3. **Columnas faltantes**: Verificar estructura de bases de datos
4. **Mapeo de IDs**: Asegurar que dimensiones se carguen primero

### Logs y Debugging
El sistema incluye mensajes informativos detallados:
- Conteos de registros por fase
- ValidaciÃ³n de columnas disponibles
- EstadÃ­sticas de mapeo y carga

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la Licencia AGPL v3. Ver archivo `LICENSE` para mÃ¡s detalles.

---

**Nota**: Este proyecto es parte de un ejercicio acadÃ©mico para demostrar implementaciÃ³n de pipelines ETL con Python y SQL Server.
